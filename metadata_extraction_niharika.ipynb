{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcQaaDucLM_3"
      },
      "outputs": [],
      "source": [
        "!pip install PyMuPDF #PyMuPDF, imported as fitz, is used to extract text from PDF files\n",
        "!pip install transformers #effective for generating summaries due to its strong contextual understanding.\n",
        "!pip install spacy #provides a fast,accurate solution for identifying named entities, which is key for understanding roles,locations,org in documents\n",
        "!pip install gensim #designed for handling large text data efficiently, which makes it suitable for topic modeling\n",
        "!pip install keybert #leverages BERT embeddings to extract keywords from the document by finding words or phrases most similar to the overall text."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">\n",
        "\n"
      ],
      "metadata": {
        "id": "fEJvXFPUKW0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "pdf_path = '/content/HLRBO_Template_Lease_Document.pdf'\n",
        "text_content = ''\n",
        "\n",
        "with fitz.open(pdf_path) as pdf:\n",
        "    for page_num in range(len(pdf)):\n",
        "        page = pdf[page_num]\n",
        "        text_content += page.get_text()\n",
        "\n",
        "# Display the first 500 characters to confirm loading\n",
        "print(text_content[:500])\n"
      ],
      "metadata": {
        "id": "1RTxHQ2YLZdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The t5-small model is a smaller version of T5 (Text-To-Text Transfer Transformer), developed by Google Research. T5 is a versatile Transformer model that treats all NLP tasks as text-to-text problems, meaning that both the input and output are in text format."
      ],
      "metadata": {
        "id": "v9V7xmcwUtDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline #Generates concise summaries of long documents.\n",
        "\n",
        "# Load summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
        "\n",
        "# Summarize the text (using first 1000 characters for brevity in demo)\n",
        "summary = summarizer(text_content[:1000], max_length=100, min_length=30, do_sample=False)\n",
        "\n",
        "# Print summary\n",
        "print(\"Summary:\")\n",
        "print(summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "id": "S-y3NDgsLgUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Topic Detection,  gensim is a library designed for topic modeling, document similarity analysis\n",
        "from gensim import corpora # corpora.Dictionary class converts the list of words (tokens) into a dictionary format,mapping each unique word to an integer ID.\n",
        "#This dictionary is essential for converting text into a format usable by the LDA model.\n",
        "from gensim.models import LdaModel #identifies patterns of co-occurrence among words to discover topics in the document.\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Preprocess text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [word.lower() for word in word_tokenize(text_content) if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "# Create dictionary and corpus for LDA\n",
        "dictionary = corpora.Dictionary([tokens])\n",
        "corpus = [dictionary.doc2bow(tokens)] # \"bag of words\" (BoW) representation captures frequency of each word in a document without considering word order\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10) # list of words with associated probabilities for each topic\n",
        "\n",
        "# Print topics\n",
        "print(\"Topics Detected:\")\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f\"Topic {idx + 1}: {topic}\")\n"
      ],
      "metadata": {
        "id": "Gyu1gca0Lj7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic 1 could be labeled \"Lease Obligations and Parties\" due to the high weights for words like \"lessee,\" \"lessor,\" and \"lease.\"\n",
        "\n",
        "Topic 2 might relate to \"Agreement Conditions\" or \"Contractual Terms\" as it contains terms like \"agreement\" and \"premises.\"\n",
        "\n",
        "The LDA model doesn’t provide a single-word label but rather a distribution of words that allows you to infer what the topic represents."
      ],
      "metadata": {
        "id": "d66Wz1AkbSNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An n-gram is a sequence of n words:\n",
        "\n",
        "1-gram (Unigram): A single word (e.g., \"agreement,\" \"lease\").\n",
        "2-gram (Bigram): A two-word phrase (e.g., \"lease agreement,\" \"landowner hereinafter\")."
      ],
      "metadata": {
        "id": "AreeNQWWcoKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Keywords\n",
        "from keybert import KeyBERT # identifies the most relevant keywords and key phrases in a text\n",
        "\n",
        "# Initialize KeyBERT\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Extract keywords\n",
        "keywords = kw_model.extract_keywords(text_content[:1000], keyphrase_ngram_range=(1, 2), stop_words='english', top_n=10)\n",
        "\n",
        "# Display keywords\n",
        "print(\"Keywords:\")\n",
        "for kw in keywords:\n",
        "    print(kw[0])\n"
      ],
      "metadata": {
        "id": "nXJFuJQGLvj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy #spaCy’s NER model is highly effective at identifying structured entities within text from unstructured documents.\n",
        "\n",
        "# Load spaCy NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text_content)\n",
        "\n",
        "# Extract and print entities\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} - {ent.label_}\")\n"
      ],
      "metadata": {
        "id": "m1D0zdA7Lw7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display final metadata summary with new lines for better readability\n",
        "metadata = {\n",
        "    \"Summary\": summary[0]['summary_text'],\n",
        "    \"Topics\": [topic[1] for topic in lda_model.print_topics(-1)],\n",
        "    \"Keywords\": [kw[0] for kw in keywords],\n",
        "    \"Named Entities\": [(ent.text, ent.label_) for ent in doc.ents]\n",
        "}\n",
        "\n",
        "print(\"\\nExtracted Metadata:\")\n",
        "for key, value in metadata.items():\n",
        "    print(f\"{key}:\\n{value}\\n\")  # Added newline after each metadata item\n",
        "\n"
      ],
      "metadata": {
        "id": "2t9QZgw-L1V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK resources if not already done\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Convert the keywords list to lowercase for consistency\n",
        "keyword_list = [kw[0].lower() for kw in keywords]\n",
        "print(\"Extracted Keywords:\", keyword_list)  # Debugging output\n",
        "\n",
        "# Tokenize the document content and also convert to lowercase\n",
        "tokens = [word.lower() for word in word_tokenize(text_content) if word.isalpha()]\n",
        "print(\"Tokens from Document:\", tokens[:50])  # Show first 50 tokens for reference\n",
        "\n",
        "# Filter tokens to include only those that match extracted keywords\n",
        "filtered_tokens = [token for token in tokens if token in keyword_list]\n",
        "print(\"Filtered Tokens (Matching Keywords):\", filtered_tokens)  # Debugging output\n",
        "\n",
        "# Count occurrences of each keyword in the filtered token list\n",
        "keyword_counts = Counter(filtered_tokens)\n",
        "\n",
        "# Display keyword frequencies\n",
        "print(\"Keyword Frequencies:\")\n",
        "for keyword, count in keyword_counts.items():\n",
        "    print(f\"{keyword}: {count}\")\n",
        "\n",
        "# If no keyword frequencies are displayed, the keywords may not be present in the document.\n"
      ],
      "metadata": {
        "id": "ApnVvYsSMbnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count approximate occurrences of each keyword as a substring\n",
        "def count_approximate_matches(text, keywords):\n",
        "    counts = {}\n",
        "    for keyword in keywords:\n",
        "        # Use lowercase text and keyword for case-insensitive matching\n",
        "        counts[keyword] = text.lower().count(keyword.lower())\n",
        "    return counts\n",
        "\n",
        "# Count occurrences of each keyword (case-insensitive, as substring)\n",
        "keyword_counts = count_approximate_matches(text_content, keyword_list)\n",
        "\n",
        "# Display keyword frequencies\n",
        "print(\"Approximate Keyword Frequencies:\")\n",
        "for keyword, count in keyword_counts.items():\n",
        "    print(f\"{keyword}: {count}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JyFSEcT9OygV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to count approximate matches of each keyword by ignoring special characters and spaces\n",
        "def count_approximate_matches_flexible(text, keywords):\n",
        "    # Clean up text: remove special characters and convert to lowercase\n",
        "    text_cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
        "    counts = {}\n",
        "\n",
        "    for keyword in keywords:\n",
        "        # Clean up the keyword similarly to ignore special characters and spaces\n",
        "        keyword_cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', keyword).lower()\n",
        "\n",
        "        # Count occurrences of the cleaned keyword in the cleaned text\n",
        "        counts[keyword] = text_cleaned.count(keyword_cleaned)\n",
        "\n",
        "    return counts\n",
        "\n",
        "# Count occurrences with flexible matching\n",
        "keyword_counts = count_approximate_matches_flexible(text_content, keyword_list)\n",
        "\n",
        "# Display keyword frequencies\n",
        "print(\"Approximate Keyword Frequencies (Flexible):\")\n",
        "for keyword, count in keyword_counts.items():\n",
        "    print(f\"{keyword}: {count}\")\n"
      ],
      "metadata": {
        "id": "77qdL0OMJg5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize sentiment analysis\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Get sentiment for the document (or summarize sections)\n",
        "sentiment = sentiment_analyzer(text_content[:1000])\n",
        "\n",
        "print(\"Sentiment Analysis:\")\n",
        "print(sentiment[0])\n"
      ],
      "metadata": {
        "id": "Y4MwgCAHPeCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The negative sentiment score likely stems from the legal and restrictive language commonly used in documents like lease agreements. Here’s why the model may interpret it as \"negative\":\n",
        "\n",
        "Restrictive Terms: Phrases like “Lessee shall be liable for…,” “no refunds…,” and “will be forfeited” often imply penalties, limitations, and consequences, which can be perceived as negative."
      ],
      "metadata": {
        "id": "i8I2pfMhJ8uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install textstat\n",
        "\n",
        "import textstat\n",
        "\n",
        "# Calculate readability score\n",
        "readability_score = textstat.flesch_reading_ease(text_content)\n",
        "\n",
        "print(\"Readability Score (Flesch Reading Ease):\", readability_score)\n"
      ],
      "metadata": {
        "id": "Fp-cQoVlPttX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This score helps us understand the level of reading skill required to comprehend the document, indicating how accessible or complex the content is for readers.\n",
        "Given the score of 46.03, the document likely contains complex vocabulary or long sentences, common in formal, legal, or technical writing"
      ],
      "metadata": {
        "id": "0_yhTn7uLDXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Initialize model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "query = \"lease agreement\"\n",
        "\n",
        "# Embed document sentences\n",
        "sentences = text_content.split('\\n')\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "# Embed query and find similar sentences\n",
        "query_embedding = model.encode(query)\n",
        "similarities = util.cos_sim(query_embedding, sentence_embeddings)\n",
        "\n",
        "# Display top 5 most similar sentences\n",
        "top_sentences = sorted(enumerate(similarities[0]), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "print(\"Most Relevant Sentences:\")\n",
        "for idx, similarity in top_sentences:\n",
        "    print(sentences[idx].strip())\n"
      ],
      "metadata": {
        "id": "jlLhdHchP4zz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}